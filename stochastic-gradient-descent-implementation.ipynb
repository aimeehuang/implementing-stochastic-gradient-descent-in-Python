{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import pdb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implemetning stochastic gradient descent in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The sizes is a list.  it contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.cost_li = []\n",
    "        self.biases = [np.random.randn(y, 1) \n",
    "                       for y in sizes[1:]] # randomly create bias vector of each node of each layer\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])] # randomly create weights of each layer\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        #pdb.set_trace()\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b) # first muliply each node(activation from previous layer) with weights then add biases\n",
    "        return a # new activation output\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desiredoutputs.\"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "    \n",
    "        #for each epoch\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            #collect cost, and accuracy of each epoch of testing data set\n",
    "            #pdb.set_trace()\n",
    "            cost_per_epoch = self.total_cost(training_data)\n",
    "            if test_data:\n",
    "                accuracy_test = self.predict(test_data)[0]/n_test\n",
    "                accuracy_train = self.predict(training_data)[0]/n\n",
    "                li = [j, mini_batch_size ,cost_per_epoch[0][0], eta, accuracy_test, accuracy_train]\n",
    "                self.cost_li.append(li)\n",
    "            \n",
    "            #shuffle the data\n",
    "            random.shuffle(training_data)       \n",
    "            # create mini batch of the training data based on predefined mini_batch size\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size] # training size of size k\n",
    "                for k in range(0, n, mini_batch_size)] # loop through size n for every mini-size batch       \n",
    "            # update weights and biases of the batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "              \n",
    "            print(\"Epoch {0} complete, loss : {1}\".format(j,cost_per_epoch))\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a batch.\n",
    "        The mini_batch is a list of tuples (x, y), and eta\n",
    "        is the learning rate.\"\"\"\n",
    "        # create empty matrix for new biases and weights\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # loop through every data point in a batch\n",
    "        for x, y in mini_batch:\n",
    "            \n",
    "            # backrpropagation :calculate derivative of cost function with respect of weights and biases\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            \n",
    "            #collect deltas of each forward\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # update weights and biases \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)] #wl→wl−η/m∑δ*(al-1).T\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)] #bl→bl−η/m∑xδx,l\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple (nabla_b, nabla_w) representing the\n",
    "        gradient for the cost function.  nabla_b and\n",
    "        nabla_w are layer-by-layer lists of numpy arrays.\"\"\"\n",
    "        # create gradient empty matrix of bias and weights\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward : sigmoid\n",
    "        #pdb.set_trace()\n",
    "        activation = x.reshape(8,1) # 8 x 1\n",
    "        activations = [x.reshape(8,1)] # list to store all the activations in every layer\n",
    "        zs = [] # list to store all the z vectors in every layer\n",
    "        \n",
    "        # calculate output using initialized/updated bias and weights\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #pdb.set_trace()\n",
    "            z = np.dot(w, activation)+b # z=(wl*al−1+bl)\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z) # al=σ(z)\n",
    "            activations.append(activation)\n",
    "     \n",
    "        # backward pass : start from the last layer\n",
    "        delta = self.cost_derivative(activations[-1], y)*sigmoid_prime(zs[-1]) # derivative of output layer:δL=(aL−y)⊙σ′(zL)\"\n",
    "        nabla_b[-1] = delta # b = δL \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # ∂C/∂wljk=(aL−1k)*δL\n",
    "        \n",
    "        \"\"\"l = 1 means the last layer of neurons, l = 2 is the second-last layer.\n",
    "          Loop backwards from L-1 layer to calculate delta of each layer\"\"\"\n",
    "        for l in range(2, self.num_layers):\n",
    "            # pdb.set_trace()\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp # δl = ((wl+1).T *δl+1)⊙σ′(zl)\n",
    "            nabla_b[-l] = delta # b = δl\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #∂C/∂wljk = (al−1k)*(δlj)\n",
    "            \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    # cost for single data sample\n",
    "    def cost_func(self, output_activations, y):\n",
    "        c =(y- output_activations)**2\n",
    "        return c\n",
    "    \n",
    "    # use MSE as cost function\n",
    "    def total_cost(self, data):\n",
    "        \"\"\"Return the total cost for the training data set ``data``\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        #loop over each data in training data set\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x.reshape(8,1))\n",
    "            #pdb.set_trace()\n",
    "            cost += self.cost_func(a, y) #sum up all the cost of each sample\n",
    "        cost = cost/len(data) # calculate mean of the cost \n",
    "       \n",
    "        return cost\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        #pdb.set_trace()\n",
    "        test_results = [(np.round(self.feedforward(x.reshape(8,1))), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return (sum(int(x == y) for (x, y) in test_results),test_results)\n",
    "    \n",
    "    def predict_prob(self, test_data):\n",
    "        \"\"\"Return the probabilities of each prediction\"\"\"\n",
    "        #pdb.set_trace()\n",
    "        test_results = [(self.feedforward(x.reshape(8,1)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return test_results\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \n",
    "        \"\"\"Return the vector of partial derivatives of cost function,\n",
    "        partial for the output activations.\"\"\"\n",
    "        \n",
    "        return (output_activations-y)\n",
    "\n",
    "# other functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def loadData(filePath):\n",
    "    \"\"\"Load data and save as numpy array\"\"\"\n",
    "    dataRaw = []\n",
    "    labelsRaw = []\n",
    "    DataFile = open(filePath)\n",
    "\n",
    "    while True:\n",
    "        theline = DataFile.readline()\n",
    "\n",
    "        if len(theline) == 0:\n",
    "            break\n",
    "\n",
    "        theline = theline.rstrip()\n",
    "\n",
    "        readData = theline.split(\",\")\n",
    "        #appending variables\n",
    "        for pos in range(len(readData)-1):\n",
    "            readData[pos] = readData[pos]; \n",
    "        dataRaw.append(np.array(readData[0:8],'float64'))\n",
    "        \n",
    "        #appending target variables\n",
    "        labelsRaw.append(np.array(readData[-1],'float64'))\n",
    "        \n",
    "    DataFile.close()\n",
    "    data = dataRaw\n",
    "    labels = labelsRaw\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def accuracy(pred, labels):\n",
    "    \"\"\"Input the predicted label and true label, it will return \n",
    "    accuracy, precision, recall, and f1 score.\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, pred).ravel()\n",
    "    accuray    = (tn+tp)/(tn+ fp +fn +tp)\n",
    "    precision  = tp/(tp+fp) \n",
    "    recall     = tp/(tp+fn)\n",
    "    f1         = (2*(precision*recall))/(precision+recall)\n",
    "\n",
    "    return(round(accuray,3) ,round(precision,3), round(recall,3) , round(f1,3))\n",
    "\n",
    "def zip_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Input is the splitted training and testing data,\n",
    "    output will be \"\"\"\n",
    "    training_data = list(zip(X_train, y_train))\n",
    "    test_data = list(zip(X_test, y_test))\n",
    "    return(training_data,test_data)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network using Pytorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnTrain_batch(X, Y, batch_size, n_epochs, learning_rate, h= 60):\n",
    "    training_Y = torch.tensor(Y, dtype = torch.float32)\n",
    "    training_X = torch.from_numpy(X)\n",
    "    loss_li=[]\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(8,h),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(h,1),\n",
    "        torch.nn.Sigmoid(),\n",
    "    )\n",
    "    # define loss function\n",
    "    loss_fn = torch.nn.MSELoss(reduction = 'mean')\n",
    "    learning_rate = learning_rate\n",
    "    n_epochs = n_epochs\n",
    "    \n",
    "    #training model in mini batch\n",
    "    for t in range(n_epochs):\n",
    "        \n",
    "        np.random.shuffle(X)        \n",
    "        current_batch = 0   \n",
    "        for iteration in range(len(Y) // batch_size):\n",
    "            # split batch\n",
    "            #pdb.set_trace()\n",
    "            batch_x = X[current_batch: current_batch + batch_size]\n",
    "            batch_y = Y[current_batch: current_batch + batch_size]\n",
    "            current_batch += batch_size\n",
    "            # convert data type\n",
    "            batch_tensor_y = torch.tensor(batch_y, dtype = torch.float32)\n",
    "            batch_tensor_x = torch.from_numpy(batch_x)\n",
    "\n",
    "            y_pred = model(batch_tensor_x)\n",
    "            \n",
    "            # input is the second value or pred : probability of class 2\n",
    "            loss = loss_fn(y_pred, batch_tensor_y)\n",
    "            #print(t, loss.item())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "        print(\"epoch: \", t,\" finish\")\n",
    "            \n",
    "        #calculate loss per epoch\n",
    "        loss_per_epoch = loss_fn(model(training_X), training_Y)\n",
    "        loss_li.append(loss_per_epoch)\n",
    "        \n",
    "        #back proprogation\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate*param.grad\n",
    "                \n",
    "    return model,loss_li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data size: 17898\n"
     ]
    }
   ],
   "source": [
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "print(\"all data size:\",len(data))\n",
    "\n",
    "#split data to training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels , test_size=0.2)\n",
    "\n",
    "#convert data into tuple\n",
    "training_inputs = [x for x in X_train]\n",
    "training_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implemented NN : test the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Epoch 4 complete\n",
      "Epoch 5 complete\n",
      "Epoch 6 complete\n",
      "Epoch 7 complete\n",
      "Epoch 8 complete\n",
      "Epoch 9 complete\n",
      "training compute time  0.008598\n",
      "1/ 2, accuracy :0.5\n",
      "accuracy: 0.5 , precision:  0.5 , recall: 1.0 ,f1: 0.667\n",
      "[[0, 100, 0.4940668160873278, 0.05, 0.5, 0.5], [1, 100, 0.49405959847111497, 0.05, 0.5, 0.5], [2, 100, 0.4940523643211362, 0.05, 0.5, 0.5], [3, 100, 0.49404511358185216, 0.05, 0.5, 0.5], [4, 100, 0.49403784619747687, 0.05, 0.5, 0.5], [5, 100, 0.4940305621119777, 0.05, 0.5, 0.5], [6, 100, 0.4940232612690724, 0.05, 0.5, 0.5], [7, 100, 0.4940159436122289, 0.05, 0.5, 0.5], [8, 100, 0.49400860908466343, 0.05, 0.5, 0.5], [9, 100, 0.4940012576293395, 0.05, 0.5, 0.5]]\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "start=datetime.now()\n",
    "net = Network([8, 10, 1])\n",
    "net.SGD(training_data, 10, 100, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "print (\"training compute time \", (datetime.now()-start).total_seconds())\n",
    "\n",
    "#validation\n",
    "accurate,results = net.predict(test_data)\n",
    "print(\"{0}/ {1}, accuracy :{2}\".format(accurate,len(test_data), accurate/ len(test_data)))\n",
    "results_unnest = [(int(x[0][0]), int(y)) for (x,y) in results]\n",
    "results_df = pd.DataFrame(results_unnest) # pred, y\n",
    "accuracy_test ,precision, recall ,f1 =  accuracy(results_df[0], results_df[1]) # pred, y\n",
    "print(\"accuracy:\",accuracy_test , \", precision: \",precision,\", recall:\", recall, \",f1:\", f1)\n",
    "\n",
    "#cost\n",
    "print(net.cost_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : training and testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "training compute time  4.341415\n",
      "tensor([0.]) tensor([0.1527])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1535])\n",
      "tensor([0.]) tensor([0.1650])\n",
      "tensor([0.]) tensor([0.1623])\n",
      "tensor([0.]) tensor([0.1475])\n",
      "tensor([0.]) tensor([0.1646])\n",
      "tensor([0.]) tensor([0.1460])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1500])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.3052])\n",
      "tensor([0.]) tensor([0.1448])\n",
      "tensor([0.]) tensor([0.1988])\n",
      "tensor([0.]) tensor([0.1576])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1638])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1498])\n",
      "tensor([0.]) tensor([0.1279])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1411])\n",
      "tensor([0.]) tensor([0.2059])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1378])\n",
      "tensor([0.]) tensor([0.1608])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1479])\n",
      "tensor([0.]) tensor([0.1402])\n",
      "tensor([0.]) tensor([0.1555])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1466])\n",
      "tensor([0.]) tensor([0.2548])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1345])\n",
      "tensor([0.]) tensor([0.2737])\n",
      "tensor([0.]) tensor([0.3117])\n",
      "tensor([0.]) tensor([0.1626])\n",
      "tensor([0.]) tensor([0.3082])\n",
      "tensor([0.]) tensor([0.1551])\n",
      "tensor([0.]) tensor([0.1485])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1571])\n",
      "tensor([0.]) tensor([0.1541])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1354])\n",
      "tensor([0.]) tensor([0.1323])\n",
      "tensor([0.]) tensor([0.1352])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1322])\n",
      "tensor([0.]) tensor([0.2956])\n",
      "tensor([0.]) tensor([0.1495])\n",
      "tensor([0.]) tensor([0.2955])\n",
      "tensor([0.]) tensor([0.1473])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1420])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1648])\n",
      "tensor([0.]) tensor([0.1309])\n",
      "tensor([0.]) tensor([0.1344])\n",
      "tensor([0.]) tensor([0.1597])\n",
      "tensor([0.]) tensor([0.1399])\n",
      "tensor([0.]) tensor([0.1303])\n",
      "tensor([0.]) tensor([0.1588])\n",
      "tensor([0.]) tensor([0.1551])\n",
      "tensor([0.]) tensor([0.1298])\n",
      "tensor([0.]) tensor([0.1314])\n",
      "tensor([0.]) tensor([0.2115])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1341])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1477])\n",
      "tensor([0.]) tensor([0.1498])\n",
      "tensor([0.]) tensor([0.1526])\n",
      "tensor([0.]) tensor([0.1465])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.1511])\n",
      "tensor([0.]) tensor([0.1527])\n",
      "tensor([0.]) tensor([0.1447])\n",
      "tensor([0.]) tensor([0.1643])\n",
      "tensor([0.]) tensor([0.1360])\n",
      "tensor([0.]) tensor([0.1637])\n",
      "tensor([0.]) tensor([0.1583])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1350])\n",
      "tensor([0.]) tensor([0.1520])\n",
      "tensor([0.]) tensor([0.1716])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1499])\n",
      "tensor([0.]) tensor([0.1275])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1560])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1392])\n",
      "tensor([0.]) tensor([0.1506])\n",
      "tensor([0.]) tensor([0.1319])\n",
      "tensor([0.]) tensor([0.1384])\n",
      "tensor([0.]) tensor([0.1522])\n",
      "tensor([0.]) tensor([0.1493])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1427])\n",
      "tensor([0.]) tensor([0.1294])\n",
      "tensor([0.]) tensor([0.1298])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1437])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1410])\n",
      "tensor([0.]) tensor([0.1606])\n",
      "tensor([0.]) tensor([0.2099])\n",
      "tensor([0.]) tensor([0.1382])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.2610])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1264])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1411])\n",
      "tensor([0.]) tensor([0.1696])\n",
      "tensor([0.]) tensor([0.1320])\n",
      "tensor([0.]) tensor([0.1821])\n",
      "tensor([0.]) tensor([0.1422])\n",
      "tensor([0.]) tensor([0.1327])\n",
      "tensor([0.]) tensor([0.1489])\n",
      "tensor([0.]) tensor([0.1411])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1546])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.1301])\n",
      "tensor([0.]) tensor([0.1438])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1547])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1807])\n",
      "tensor([0.]) tensor([0.1353])\n",
      "tensor([0.]) tensor([0.1348])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1550])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1622])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1495])\n",
      "tensor([0.]) tensor([0.1549])\n",
      "tensor([0.]) tensor([0.3117])\n",
      "tensor([0.]) tensor([0.1378])\n",
      "tensor([0.]) tensor([0.3110])\n",
      "tensor([0.]) tensor([0.1381])\n",
      "tensor([0.]) tensor([0.1703])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1792])\n",
      "tensor([0.]) tensor([0.1602])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1490])\n",
      "tensor([0.]) tensor([0.2767])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1533])\n",
      "tensor([0.]) tensor([0.1539])\n",
      "tensor([0.]) tensor([0.2570])\n",
      "tensor([0.]) tensor([0.1465])\n",
      "tensor([0.]) tensor([0.1408])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1426])\n",
      "tensor([0.]) tensor([0.1453])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.1483])\n",
      "tensor([0.]) tensor([0.1578])\n",
      "tensor([0.]) tensor([0.1373])\n",
      "tensor([0.]) tensor([0.1357])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1367])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1358])\n",
      "tensor([0.]) tensor([0.1375])\n",
      "tensor([0.]) tensor([0.1571])\n",
      "tensor([0.]) tensor([0.1340])\n",
      "tensor([0.]) tensor([0.1530])\n",
      "tensor([0.]) tensor([0.1346])\n",
      "tensor([0.]) tensor([0.1413])\n",
      "tensor([0.]) tensor([0.1296])\n",
      "tensor([0.]) tensor([0.1391])\n",
      "tensor([0.]) tensor([0.1475])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1309])\n",
      "tensor([0.]) tensor([0.1492])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1462])\n",
      "tensor([0.]) tensor([0.3128])\n",
      "tensor([0.]) tensor([0.2394])\n",
      "tensor([0.]) tensor([0.1750])\n",
      "tensor([0.]) tensor([0.1341])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1701])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1577])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.2793])\n",
      "tensor([0.]) tensor([0.1518])\n",
      "tensor([0.]) tensor([0.2358])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1705])\n",
      "tensor([0.]) tensor([0.1457])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1330])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1603])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1468])\n",
      "tensor([0.]) tensor([0.1260])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.3454])\n",
      "tensor([0.]) tensor([0.2566])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1458])\n",
      "tensor([0.]) tensor([0.1463])\n",
      "tensor([0.]) tensor([0.1505])\n",
      "tensor([0.]) tensor([0.1293])\n",
      "tensor([0.]) tensor([0.1760])\n",
      "tensor([0.]) tensor([0.1457])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1324])\n",
      "tensor([0.]) tensor([0.1572])\n",
      "tensor([0.]) tensor([0.1485])\n",
      "tensor([0.]) tensor([0.1363])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1515])\n",
      "tensor([0.]) tensor([0.3101])\n",
      "tensor([0.]) tensor([0.2931])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1311])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1317])\n",
      "tensor([0.]) tensor([0.1279])\n",
      "tensor([0.]) tensor([0.2922])\n",
      "tensor([0.]) tensor([0.1579])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1363])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.2340])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1452])\n",
      "tensor([0.]) tensor([0.1348])\n",
      "tensor([0.]) tensor([0.1372])\n",
      "tensor([0.]) tensor([0.1633])\n",
      "tensor([0.]) tensor([0.2812])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.2103])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1626])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1384])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1529])\n",
      "tensor([0.]) tensor([0.1291])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1433])\n",
      "tensor([0.]) tensor([0.1567])\n",
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1323])\n",
      "tensor([0.]) tensor([0.1385])\n",
      "tensor([0.]) tensor([0.1705])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2872])\n",
      "tensor([0.]) tensor([0.1500])\n",
      "tensor([0.]) tensor([0.1524])\n",
      "tensor([0.]) tensor([0.1428])\n",
      "tensor([0.]) tensor([0.1444])\n",
      "tensor([0.]) tensor([0.1902])\n",
      "tensor([0.]) tensor([0.1507])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1508])\n",
      "tensor([0.]) tensor([0.1336])\n",
      "tensor([0.]) tensor([0.1610])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1423])\n",
      "tensor([0.]) tensor([0.1311])\n",
      "tensor([0.]) tensor([0.1453])\n",
      "tensor([0.]) tensor([0.1330])\n",
      "tensor([0.]) tensor([0.1362])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1676])\n",
      "tensor([0.]) tensor([0.1350])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1444])\n",
      "tensor([0.]) tensor([0.1671])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1455])\n",
      "tensor([0.]) tensor([0.1299])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1612])\n",
      "tensor([0.]) tensor([0.3290])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1565])\n",
      "tensor([0.]) tensor([0.1385])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.1433])\n",
      "tensor([0.]) tensor([0.1560])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1313])\n",
      "tensor([0.]) tensor([0.2572])\n",
      "tensor([0.]) tensor([0.2178])\n",
      "tensor([0.]) tensor([0.1777])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1483])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.2931])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.2247])\n",
      "tensor([0.]) tensor([0.1711])\n",
      "tensor([0.]) tensor([0.1553])\n",
      "tensor([0.]) tensor([0.1455])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1334])\n",
      "tensor([0.]) tensor([0.2428])\n",
      "tensor([0.]) tensor([0.2544])\n",
      "tensor([0.]) tensor([0.1518])\n",
      "tensor([0.]) tensor([0.2163])\n",
      "tensor([0.]) tensor([0.2140])\n",
      "tensor([0.]) tensor([0.2024])\n",
      "tensor([0.]) tensor([0.2577])\n",
      "tensor([0.]) tensor([0.2069])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1602])\n",
      "tensor([0.]) tensor([0.3030])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.3033])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1927])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1372])\n",
      "tensor([0.]) tensor([0.1984])\n",
      "tensor([0.]) tensor([0.2205])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1639])\n",
      "tensor([0.]) tensor([0.2467])\n",
      "tensor([0.]) tensor([0.2639])\n",
      "tensor([0.]) tensor([0.1915])\n",
      "tensor([0.]) tensor([0.1463])\n",
      "tensor([0.]) tensor([0.1483])\n",
      "tensor([0.]) tensor([0.1537])\n",
      "tensor([0.]) tensor([0.2281])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1451])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.3125])\n",
      "tensor([0.]) tensor([0.1533])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.2876])\n",
      "tensor([0.]) tensor([0.1321])\n",
      "tensor([0.]) tensor([0.1505])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1310])\n",
      "tensor([0.]) tensor([0.1939])\n",
      "tensor([0.]) tensor([0.1667])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1475])\n",
      "tensor([0.]) tensor([0.1320])\n",
      "tensor([0.]) tensor([0.1568])\n",
      "tensor([0.]) tensor([0.1399])\n",
      "tensor([0.]) tensor([0.1328])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1584])\n",
      "tensor([0.]) tensor([0.2596])\n",
      "tensor([0.]) tensor([0.1660])\n",
      "tensor([0.]) tensor([0.2881])\n",
      "tensor([0.]) tensor([0.1404])\n",
      "tensor([0.]) tensor([0.2035])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.2802])\n",
      "tensor([0.]) tensor([0.1387])\n",
      "tensor([0.]) tensor([0.1948])\n",
      "tensor([0.]) tensor([0.2883])\n",
      "tensor([0.]) tensor([0.1514])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1362])\n",
      "tensor([0.]) tensor([0.1531])\n",
      "tensor([0.]) tensor([0.1407])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1494])\n",
      "tensor([0.]) tensor([0.3225])\n",
      "tensor([0.]) tensor([0.1633])\n",
      "tensor([0.]) tensor([0.1416])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1417])\n",
      "tensor([0.]) tensor([0.1455])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1417])\n",
      "tensor([0.]) tensor([0.1610])\n",
      "tensor([0.]) tensor([0.1445])\n",
      "tensor([0.]) tensor([0.1623])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2852])\n",
      "tensor([0.]) tensor([0.1482])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1599])\n",
      "tensor([0.]) tensor([0.3182])\n",
      "tensor([0.]) tensor([0.3105])\n",
      "tensor([0.]) tensor([0.1294])\n",
      "tensor([0.]) tensor([0.1491])\n",
      "tensor([0.]) tensor([0.1407])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1311])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1777])\n",
      "tensor([0.]) tensor([0.1340])\n",
      "tensor([0.]) tensor([0.2421])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.2106])\n",
      "tensor([0.]) tensor([0.2595])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1475])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1508])\n",
      "tensor([0.]) tensor([0.1555])\n",
      "tensor([0.]) tensor([0.2815])\n",
      "tensor([0.]) tensor([0.1501])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1427])\n",
      "tensor([0.]) tensor([0.1482])\n",
      "tensor([0.]) tensor([0.1293])\n",
      "tensor([0.]) tensor([0.1606])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1520])\n",
      "tensor([0.]) tensor([0.1463])\n",
      "tensor([0.]) tensor([0.1438])\n",
      "tensor([0.]) tensor([0.1594])\n",
      "tensor([0.]) tensor([0.1635])\n",
      "tensor([0.]) tensor([0.1441])\n",
      "tensor([0.]) tensor([0.1370])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1489])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.3066])\n",
      "tensor([0.]) tensor([0.1360])\n",
      "tensor([0.]) tensor([0.1538])\n",
      "tensor([0.]) tensor([0.1570])\n",
      "tensor([0.]) tensor([0.2890])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.2813])\n",
      "tensor([0.]) tensor([0.1895])\n",
      "tensor([0.]) tensor([0.1827])\n",
      "tensor([0.]) tensor([0.1356])\n",
      "tensor([0.]) tensor([0.1530])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.2309])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1423])\n",
      "tensor([0.]) tensor([0.1431])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1381])\n",
      "tensor([0.]) tensor([0.1261])\n",
      "tensor([0.]) tensor([0.2421])\n",
      "tensor([0.]) tensor([0.2764])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1507])\n",
      "tensor([0.]) tensor([0.1544])\n",
      "tensor([0.]) tensor([0.1569])\n",
      "tensor([0.]) tensor([0.1407])\n",
      "tensor([0.]) tensor([0.1588])\n",
      "tensor([0.]) tensor([0.1361])\n",
      "tensor([0.]) tensor([0.1753])\n",
      "tensor([0.]) tensor([0.1559])\n",
      "tensor([0.]) tensor([0.1539])\n",
      "tensor([0.]) tensor([0.1589])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.2823])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1562])\n",
      "tensor([0.]) tensor([0.1262])\n",
      "tensor([0.]) tensor([0.1362])\n",
      "tensor([0.]) tensor([0.3015])\n",
      "tensor([0.]) tensor([0.2774])\n",
      "tensor([0.]) tensor([0.1299])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1545])\n",
      "tensor([0.]) tensor([0.1507])\n",
      "tensor([0.]) tensor([0.1377])\n",
      "tensor([0.]) tensor([0.1452])\n",
      "tensor([0.]) tensor([0.1579])\n",
      "tensor([0.]) tensor([0.1308])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.2555])\n",
      "tensor([0.]) tensor([0.1685])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.1686])\n",
      "tensor([0.]) tensor([0.1343])\n",
      "tensor([0.]) tensor([0.1349])\n",
      "tensor([0.]) tensor([0.1411])\n",
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1561])\n",
      "tensor([0.]) tensor([0.3066])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1331])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.2732])\n",
      "tensor([0.]) tensor([0.1512])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2609])\n",
      "tensor([0.]) tensor([0.1490])\n",
      "tensor([0.]) tensor([0.1423])\n",
      "tensor([0.]) tensor([0.1366])\n",
      "tensor([0.]) tensor([0.2984])\n",
      "tensor([0.]) tensor([0.1465])\n",
      "tensor([0.]) tensor([0.2886])\n",
      "tensor([0.]) tensor([0.2953])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.1509])\n",
      "tensor([0.]) tensor([0.3075])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1350])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.2458])\n",
      "tensor([0.]) tensor([0.2078])\n",
      "tensor([0.]) tensor([0.1397])\n",
      "tensor([0.]) tensor([0.2290])\n",
      "tensor([0.]) tensor([0.1422])\n",
      "tensor([0.]) tensor([0.1606])\n",
      "tensor([0.]) tensor([0.1440])\n",
      "tensor([0.]) tensor([0.1639])\n",
      "tensor([0.]) tensor([0.2897])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1511])\n",
      "tensor([0.]) tensor([0.1568])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1538])\n",
      "tensor([0.]) tensor([0.1515])\n",
      "tensor([0.]) tensor([0.2124])\n",
      "tensor([0.]) tensor([0.3082])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2696])\n",
      "tensor([0.]) tensor([0.3169])\n",
      "tensor([0.]) tensor([0.1659])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2297])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.1547])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1309])\n",
      "tensor([0.]) tensor([0.1374])\n",
      "tensor([0.]) tensor([0.2481])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1535])\n",
      "tensor([0.]) tensor([0.1292])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1425])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1298])\n",
      "tensor([0.]) tensor([0.1409])\n",
      "tensor([0.]) tensor([0.1550])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1404])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1349])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.1525])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.2713])\n",
      "tensor([0.]) tensor([0.2373])\n",
      "tensor([0.]) tensor([0.1524])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1492])\n",
      "tensor([0.]) tensor([0.1279])\n",
      "tensor([0.]) tensor([0.1625])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1386])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1583])\n",
      "tensor([0.]) tensor([0.1458])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.1308])\n",
      "tensor([0.]) tensor([0.1588])\n",
      "tensor([0.]) tensor([0.1410])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.2135])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1288])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.2337])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.2086])\n",
      "tensor([0.]) tensor([0.1263])\n",
      "tensor([0.]) tensor([0.1645])\n",
      "tensor([0.]) tensor([0.1662])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1288])\n",
      "tensor([0.]) tensor([0.1465])\n",
      "tensor([0.]) tensor([0.1286])\n",
      "tensor([0.]) tensor([0.1491])\n",
      "tensor([0.]) tensor([0.1686])\n",
      "tensor([0.]) tensor([0.2027])\n",
      "tensor([0.]) tensor([0.1285])\n",
      "tensor([0.]) tensor([0.1622])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1291])\n",
      "tensor([0.]) tensor([0.1492])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.1542])\n",
      "tensor([0.]) tensor([0.1542])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1590])\n",
      "tensor([0.]) tensor([0.1697])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1518])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1385])\n",
      "tensor([0.]) tensor([0.1515])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.2163])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1401])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1806])\n",
      "tensor([0.]) tensor([0.1511])\n",
      "tensor([0.]) tensor([0.1601])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1361])\n",
      "tensor([0.]) tensor([0.1859])\n",
      "tensor([0.]) tensor([0.1748])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1351])\n",
      "tensor([0.]) tensor([0.1692])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.1727])\n",
      "tensor([0.]) tensor([0.1377])\n",
      "tensor([0.]) tensor([0.2056])\n",
      "tensor([0.]) tensor([0.1592])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1320])\n",
      "tensor([0.]) tensor([0.1542])\n",
      "tensor([0.]) tensor([0.1308])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.1397])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1997])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.1368])\n",
      "tensor([0.]) tensor([0.1386])\n",
      "tensor([0.]) tensor([0.1391])\n",
      "tensor([0.]) tensor([0.1615])\n",
      "tensor([0.]) tensor([0.1965])\n",
      "tensor([0.]) tensor([0.1465])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1433])\n",
      "tensor([0.]) tensor([0.1316])\n",
      "tensor([0.]) tensor([0.1634])\n",
      "tensor([0.]) tensor([0.2042])\n",
      "tensor([0.]) tensor([0.2146])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1506])\n",
      "tensor([0.]) tensor([0.2011])\n",
      "tensor([0.]) tensor([0.2028])\n",
      "tensor([0.]) tensor([0.1568])\n",
      "tensor([0.]) tensor([0.2095])\n",
      "tensor([0.]) tensor([0.2104])\n",
      "tensor([0.]) tensor([0.1597])\n",
      "tensor([0.]) tensor([0.2239])\n",
      "tensor([0.]) tensor([0.2129])\n",
      "tensor([0.]) tensor([0.1700])\n",
      "tensor([0.]) tensor([0.1555])\n",
      "tensor([0.]) tensor([0.2247])\n",
      "tensor([0.]) tensor([0.2103])\n",
      "tensor([0.]) tensor([0.1363])\n",
      "tensor([0.]) tensor([0.2434])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.2090])\n",
      "tensor([0.]) tensor([0.1978])\n",
      "tensor([0.]) tensor([0.2950])\n",
      "tensor([0.]) tensor([0.2511])\n",
      "tensor([0.]) tensor([0.1706])\n",
      "tensor([0.]) tensor([0.2060])\n",
      "tensor([0.]) tensor([0.1474])\n",
      "tensor([0.]) tensor([0.2509])\n",
      "tensor([0.]) tensor([0.1673])\n",
      "tensor([0.]) tensor([0.1574])\n",
      "tensor([0.]) tensor([0.2874])\n",
      "tensor([0.]) tensor([0.1580])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1356])\n",
      "tensor([0.]) tensor([0.2236])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1626])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1525])\n",
      "tensor([0.]) tensor([0.1516])\n",
      "tensor([0.]) tensor([0.1599])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.3098])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1363])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1392])\n",
      "tensor([0.]) tensor([0.1266])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1640])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2171])\n",
      "tensor([0.]) tensor([0.1889])\n",
      "tensor([0.]) tensor([0.2472])\n",
      "tensor([0.]) tensor([0.1923])\n",
      "tensor([0.]) tensor([0.1282])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.3444])\n",
      "tensor([0.]) tensor([0.1445])\n",
      "tensor([0.]) tensor([0.1312])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1631])\n",
      "tensor([0.]) tensor([0.1548])\n",
      "tensor([0.]) tensor([0.1328])\n",
      "tensor([0.]) tensor([0.1593])\n",
      "tensor([0.]) tensor([0.2748])\n",
      "tensor([0.]) tensor([0.1640])\n",
      "tensor([0.]) tensor([0.1833])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.2106])\n",
      "tensor([0.]) tensor([0.1822])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1411])\n",
      "tensor([0.]) tensor([0.3031])\n",
      "tensor([0.]) tensor([0.1634])\n",
      "tensor([0.]) tensor([0.1402])\n",
      "tensor([0.]) tensor([0.1404])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1609])\n",
      "tensor([0.]) tensor([0.1428])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2443])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1753])\n",
      "tensor([0.]) tensor([0.3060])\n",
      "tensor([0.]) tensor([0.1326])\n",
      "tensor([0.]) tensor([0.1590])\n",
      "tensor([0.]) tensor([0.1271])\n",
      "tensor([0.]) tensor([0.2917])\n",
      "tensor([0.]) tensor([0.1533])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1647])\n",
      "tensor([0.]) tensor([0.1382])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.3098])\n",
      "tensor([0.]) tensor([0.1628])\n",
      "tensor([0.]) tensor([0.1437])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.3457])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1212])\n",
      "tensor([0.]) tensor([0.1669])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1616])\n",
      "tensor([0.]) tensor([0.1555])\n",
      "tensor([0.]) tensor([0.3006])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1700])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1558])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1310])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1286])\n",
      "tensor([0.]) tensor([0.3030])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.2992])\n",
      "tensor([0.]) tensor([0.1604])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1398])\n",
      "tensor([0.]) tensor([0.1531])\n",
      "tensor([0.]) tensor([0.1318])\n",
      "tensor([0.]) tensor([0.1395])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1539])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1461])\n",
      "tensor([0.]) tensor([0.1520])\n",
      "tensor([0.]) tensor([0.1402])\n",
      "tensor([0.]) tensor([0.1304])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.2552])\n",
      "tensor([0.]) tensor([0.3171])\n",
      "tensor([0.]) tensor([0.2405])\n",
      "tensor([0.]) tensor([0.1588])\n",
      "tensor([0.]) tensor([0.1412])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1515])\n",
      "tensor([0.]) tensor([0.2211])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1460])\n",
      "tensor([0.]) tensor([0.2555])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1447])\n",
      "tensor([0.]) tensor([0.2186])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1650])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1699])\n",
      "tensor([0.]) tensor([0.1598])\n",
      "tensor([0.]) tensor([0.1299])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1657])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.1535])\n",
      "tensor([0.]) tensor([0.1282])\n",
      "tensor([0.]) tensor([0.1585])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.3245])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1543])\n",
      "tensor([0.]) tensor([0.1458])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1375])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1479])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1404])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1564])\n",
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1740])\n",
      "tensor([0.]) tensor([0.1575])\n",
      "tensor([0.]) tensor([0.1312])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.1511])\n",
      "tensor([0.]) tensor([0.1340])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1623])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1628])\n",
      "tensor([0.]) tensor([0.1305])\n",
      "tensor([0.]) tensor([0.2204])\n",
      "tensor([0.]) tensor([0.1428])\n",
      "tensor([0.]) tensor([0.2126])\n",
      "tensor([0.]) tensor([0.1435])\n",
      "tensor([0.]) tensor([0.1423])\n",
      "tensor([0.]) tensor([0.1281])\n",
      "tensor([0.]) tensor([0.1448])\n",
      "tensor([0.]) tensor([0.1468])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1374])\n",
      "tensor([0.]) tensor([0.1354])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1455])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1468])\n",
      "tensor([0.]) tensor([0.1553])\n",
      "tensor([0.]) tensor([0.2532])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1275])\n",
      "tensor([0.]) tensor([0.1328])\n",
      "tensor([0.]) tensor([0.1548])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2457])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1566])\n",
      "tensor([0.]) tensor([0.1331])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1410])\n",
      "tensor([0.]) tensor([0.1693])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1552])\n",
      "tensor([0.]) tensor([0.1640])\n",
      "tensor([0.]) tensor([0.1599])\n",
      "tensor([0.]) tensor([0.1387])\n",
      "tensor([0.]) tensor([0.1312])\n",
      "tensor([0.]) tensor([0.1296])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1583])\n",
      "tensor([0.]) tensor([0.1563])\n",
      "tensor([0.]) tensor([0.1631])\n",
      "tensor([0.]) tensor([0.1509])\n",
      "tensor([0.]) tensor([0.2642])\n",
      "tensor([0.]) tensor([0.2857])\n",
      "tensor([0.]) tensor([0.1460])\n",
      "tensor([0.]) tensor([0.1373])\n",
      "tensor([0.]) tensor([0.1332])\n",
      "tensor([0.]) tensor([0.1660])\n",
      "tensor([0.]) tensor([0.1414])\n",
      "tensor([0.]) tensor([0.1593])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1467])\n",
      "tensor([0.]) tensor([0.1292])\n",
      "tensor([0.]) tensor([0.1666])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.2714])\n",
      "tensor([0.]) tensor([0.2062])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1541])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1499])\n",
      "tensor([0.]) tensor([0.1327])\n",
      "tensor([0.]) tensor([0.2484])\n",
      "tensor([0.]) tensor([0.1463])\n",
      "tensor([0.]) tensor([0.2944])\n",
      "tensor([0.]) tensor([0.1569])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1551])\n",
      "tensor([0.]) tensor([0.1695])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.2299])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1492])\n",
      "tensor([0.]) tensor([0.1370])\n",
      "tensor([0.]) tensor([0.1309])\n",
      "tensor([0.]) tensor([0.2034])\n",
      "tensor([0.]) tensor([0.2874])\n",
      "tensor([0.]) tensor([0.2127])\n",
      "tensor([0.]) tensor([0.1285])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1578])\n",
      "tensor([0.]) tensor([0.1571])\n",
      "tensor([0.]) tensor([0.1678])\n",
      "tensor([0.]) tensor([0.2308])\n",
      "tensor([0.]) tensor([0.1327])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.2377])\n",
      "tensor([0.]) tensor([0.1882])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1341])\n",
      "tensor([0.]) tensor([0.1366])\n",
      "tensor([0.]) tensor([0.2105])\n",
      "tensor([0.]) tensor([0.1573])\n",
      "tensor([0.]) tensor([0.2287])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1325])\n",
      "tensor([0.]) tensor([0.1311])\n",
      "tensor([0.]) tensor([0.1700])\n",
      "tensor([0.]) tensor([0.1597])\n",
      "tensor([0.]) tensor([0.1285])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1523])\n",
      "tensor([0.]) tensor([0.1569])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1473])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1286])\n",
      "tensor([0.]) tensor([0.1409])\n",
      "tensor([0.]) tensor([0.1495])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.2737])\n",
      "tensor([0.]) tensor([0.1290])\n",
      "tensor([0.]) tensor([0.1304])\n",
      "tensor([0.]) tensor([0.1431])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1422])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1504])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1359])\n",
      "tensor([0.]) tensor([0.1441])\n",
      "tensor([0.]) tensor([0.1291])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1607])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1283])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1381])\n",
      "tensor([0.]) tensor([0.1559])\n",
      "tensor([0.]) tensor([0.1386])\n",
      "tensor([0.]) tensor([0.1513])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1422])\n",
      "tensor([0.]) tensor([0.1304])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1508])\n",
      "tensor([0.]) tensor([0.1375])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1466])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1493])\n",
      "tensor([0.]) tensor([0.1311])\n",
      "tensor([0.]) tensor([0.1582])\n",
      "tensor([0.]) tensor([0.1318])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1437])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.1324])\n",
      "tensor([0.]) tensor([0.1600])\n",
      "tensor([0.]) tensor([0.1278])\n",
      "tensor([0.]) tensor([0.1477])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1296])\n",
      "tensor([0.]) tensor([0.1324])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1448])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1416])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1364])\n",
      "tensor([0.]) tensor([0.1461])\n",
      "tensor([0.]) tensor([0.2981])\n",
      "tensor([0.]) tensor([0.1283])\n",
      "tensor([0.]) tensor([0.2390])\n",
      "tensor([0.]) tensor([0.1664])\n",
      "tensor([0.]) tensor([0.2908])\n",
      "tensor([0.]) tensor([0.3405])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1282])\n",
      "tensor([0.]) tensor([0.2294])\n",
      "tensor([0.]) tensor([0.2507])\n",
      "tensor([0.]) tensor([0.1580])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1399])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1330])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1514])\n",
      "tensor([0.]) tensor([0.1545])\n",
      "tensor([0.]) tensor([0.1387])\n",
      "tensor([0.]) tensor([0.1938])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.2958])\n",
      "tensor([0.]) tensor([0.1498])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1341])\n",
      "tensor([0.]) tensor([0.2575])\n",
      "tensor([0.]) tensor([0.2978])\n",
      "tensor([0.]) tensor([0.3130])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1489])\n",
      "tensor([0.]) tensor([0.1417])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1397])\n",
      "tensor([0.]) tensor([0.2991])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1589])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1631])\n",
      "tensor([0.]) tensor([0.2294])\n",
      "tensor([0.]) tensor([0.1998])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1340])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1436])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1282])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1540])\n",
      "tensor([0.]) tensor([0.1372])\n",
      "tensor([0.]) tensor([0.1624])\n",
      "tensor([0.]) tensor([0.1358])\n",
      "tensor([0.]) tensor([0.2091])\n",
      "tensor([0.]) tensor([0.3149])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2620])\n",
      "tensor([0.]) tensor([0.2514])\n",
      "tensor([0.]) tensor([0.1567])\n",
      "tensor([0.]) tensor([0.1435])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1230])\n",
      "tensor([0.]) tensor([0.1281])\n",
      "tensor([0.]) tensor([0.1615])\n",
      "tensor([0.]) tensor([0.3024])\n",
      "tensor([0.]) tensor([0.1616])\n",
      "tensor([0.]) tensor([0.1370])\n",
      "tensor([0.]) tensor([0.1633])\n",
      "tensor([0.]) tensor([0.2911])\n",
      "tensor([0.]) tensor([0.3031])\n",
      "tensor([0.]) tensor([0.3014])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.1303])\n",
      "tensor([0.]) tensor([0.1830])\n",
      "tensor([0.]) tensor([0.1678])\n",
      "tensor([0.]) tensor([0.1372])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.2158])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.2930])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.3034])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1281])\n",
      "tensor([0.]) tensor([0.1404])\n",
      "tensor([0.]) tensor([0.1413])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1316])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.1440])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1335])\n",
      "tensor([0.]) tensor([0.1559])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1521])\n",
      "tensor([0.]) tensor([0.1691])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1412])\n",
      "tensor([0.]) tensor([0.1693])\n",
      "tensor([0.]) tensor([0.1416])\n",
      "tensor([0.]) tensor([0.1590])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1540])\n",
      "tensor([0.]) tensor([0.1448])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1437])\n",
      "tensor([0.]) tensor([0.1345])\n",
      "tensor([0.]) tensor([0.1642])\n",
      "tensor([0.]) tensor([0.1447])\n",
      "tensor([0.]) tensor([0.1392])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1329])\n",
      "tensor([0.]) tensor([0.1424])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.1344])\n",
      "tensor([0.]) tensor([0.1669])\n",
      "tensor([0.]) tensor([0.1555])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1553])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1512])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1292])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1671])\n",
      "tensor([0.]) tensor([0.1468])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1368])\n",
      "tensor([0.]) tensor([0.1319])\n",
      "tensor([0.]) tensor([0.1557])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1420])\n",
      "tensor([0.]) tensor([0.1532])\n",
      "tensor([0.]) tensor([0.1338])\n",
      "tensor([0.]) tensor([0.1259])\n",
      "tensor([0.]) tensor([0.1316])\n",
      "tensor([0.]) tensor([0.1644])\n",
      "tensor([0.]) tensor([0.1532])\n",
      "tensor([0.]) tensor([0.1510])\n",
      "tensor([0.]) tensor([0.2793])\n",
      "tensor([0.]) tensor([0.1498])\n",
      "tensor([0.]) tensor([0.1471])\n",
      "tensor([0.]) tensor([0.1316])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1596])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.2298])\n",
      "tensor([0.]) tensor([0.1286])\n",
      "tensor([0.]) tensor([0.1338])\n",
      "tensor([0.]) tensor([0.1601])\n",
      "tensor([0.]) tensor([0.1247])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1344])\n",
      "tensor([0.]) tensor([0.1958])\n",
      "tensor([0.]) tensor([0.1312])\n",
      "tensor([0.]) tensor([0.1337])\n",
      "tensor([0.]) tensor([0.1552])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2487])\n",
      "tensor([0.]) tensor([0.1693])\n",
      "tensor([0.]) tensor([0.1571])\n",
      "tensor([0.]) tensor([0.1690])\n",
      "tensor([0.]) tensor([0.1589])\n",
      "tensor([0.]) tensor([0.1409])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1395])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1721])\n",
      "tensor([0.]) tensor([0.1264])\n",
      "tensor([0.]) tensor([0.1665])\n",
      "tensor([0.]) tensor([0.2903])\n",
      "tensor([0.]) tensor([0.1390])\n",
      "tensor([0.]) tensor([0.1387])\n",
      "tensor([0.]) tensor([0.1481])\n",
      "tensor([0.]) tensor([0.1745])\n",
      "tensor([0.]) tensor([0.1393])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1743])\n",
      "tensor([0.]) tensor([0.1577])\n",
      "tensor([0.]) tensor([0.1277])\n",
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1615])\n",
      "tensor([0.]) tensor([0.1359])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1382])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1612])\n",
      "tensor([0.]) tensor([0.2405])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.1362])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1325])\n",
      "tensor([0.]) tensor([0.1461])\n",
      "tensor([0.]) tensor([0.1295])\n",
      "tensor([0.]) tensor([0.1381])\n",
      "tensor([0.]) tensor([0.1320])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1677])\n",
      "tensor([0.]) tensor([0.2595])\n",
      "tensor([0.]) tensor([0.1428])\n",
      "tensor([0.]) tensor([0.1414])\n",
      "tensor([0.]) tensor([0.1402])\n",
      "tensor([0.]) tensor([0.1502])\n",
      "tensor([0.]) tensor([0.1414])\n",
      "tensor([0.]) tensor([0.1684])\n",
      "tensor([0.]) tensor([0.1313])\n",
      "tensor([0.]) tensor([0.1283])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1482])\n",
      "tensor([0.]) tensor([0.1401])\n",
      "tensor([0.]) tensor([0.1540])\n",
      "tensor([0.]) tensor([0.2819])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2231])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1347])\n",
      "tensor([0.]) tensor([0.2608])\n",
      "tensor([0.]) tensor([0.1403])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.2046])\n",
      "tensor([0.]) tensor([0.2137])\n",
      "tensor([0.]) tensor([0.1512])\n",
      "tensor([0.]) tensor([0.2032])\n",
      "tensor([0.]) tensor([0.3053])\n",
      "tensor([0.]) tensor([0.1325])\n",
      "tensor([0.]) tensor([0.1264])\n",
      "tensor([0.]) tensor([0.1583])\n",
      "tensor([0.]) tensor([0.2043])\n",
      "tensor([0.]) tensor([0.2127])\n",
      "tensor([0.]) tensor([0.1398])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1272])\n",
      "tensor([0.]) tensor([0.1378])\n",
      "tensor([0.]) tensor([0.1426])\n",
      "tensor([0.]) tensor([0.1374])\n",
      "tensor([0.]) tensor([0.2343])\n",
      "tensor([0.]) tensor([0.2331])\n",
      "tensor([0.]) tensor([0.1277])\n",
      "tensor([0.]) tensor([0.1426])\n",
      "tensor([0.]) tensor([0.2028])\n",
      "tensor([0.]) tensor([0.2266])\n",
      "tensor([0.]) tensor([0.2116])\n",
      "tensor([0.]) tensor([0.1902])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.2062])\n",
      "tensor([0.]) tensor([0.1619])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.1543])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1443])\n",
      "tensor([0.]) tensor([0.1495])\n",
      "tensor([0.]) tensor([0.1596])\n",
      "tensor([0.]) tensor([0.1393])\n",
      "tensor([0.]) tensor([0.1667])\n",
      "tensor([0.]) tensor([0.1365])\n",
      "tensor([0.]) tensor([0.1541])\n",
      "tensor([0.]) tensor([0.2915])\n",
      "tensor([0.]) tensor([0.2788])\n",
      "tensor([0.]) tensor([0.1355])\n",
      "tensor([0.]) tensor([0.1237])\n",
      "tensor([0.]) tensor([0.2967])\n",
      "tensor([0.]) tensor([0.1391])\n",
      "tensor([0.]) tensor([0.1477])\n",
      "tensor([0.]) tensor([0.1498])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1302])\n",
      "tensor([0.]) tensor([0.1301])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1409])\n",
      "tensor([0.]) tensor([0.1537])\n",
      "tensor([0.]) tensor([0.1671])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1406])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1506])\n",
      "tensor([0.]) tensor([0.2090])\n",
      "tensor([0.]) tensor([0.1860])\n",
      "tensor([0.]) tensor([0.1326])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.2057])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1646])\n",
      "tensor([0.]) tensor([0.2808])\n",
      "tensor([0.]) tensor([0.1378])\n",
      "tensor([0.]) tensor([0.1368])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1500])\n",
      "tensor([0.]) tensor([0.2091])\n",
      "tensor([0.]) tensor([0.2024])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.2043])\n",
      "tensor([0.]) tensor([0.3034])\n",
      "tensor([0.]) tensor([0.2347])\n",
      "tensor([0.]) tensor([0.2073])\n",
      "tensor([0.]) tensor([0.1958])\n",
      "tensor([0.]) tensor([0.1515])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.2306])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2101])\n",
      "tensor([0.]) tensor([0.2897])\n",
      "tensor([0.]) tensor([0.2208])\n",
      "tensor([0.]) tensor([0.1574])\n",
      "tensor([0.]) tensor([0.1728])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.2125])\n",
      "tensor([0.]) tensor([0.1391])\n",
      "tensor([0.]) tensor([0.1483])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1693])\n",
      "tensor([0.]) tensor([0.1618])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1531])\n",
      "tensor([0.]) tensor([0.2190])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.1375])\n",
      "tensor([0.]) tensor([0.1571])\n",
      "tensor([0.]) tensor([0.1377])\n",
      "tensor([0.]) tensor([0.1277])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2105])\n",
      "tensor([0.]) tensor([0.1367])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1425])\n",
      "tensor([0.]) tensor([0.1291])\n",
      "tensor([0.]) tensor([0.1278])\n",
      "tensor([0.]) tensor([0.2373])\n",
      "tensor([0.]) tensor([0.1299])\n",
      "tensor([0.]) tensor([0.1262])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.1499])\n",
      "tensor([0.]) tensor([0.1476])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1635])\n",
      "tensor([0.]) tensor([0.1618])\n",
      "tensor([0.]) tensor([0.1483])\n",
      "tensor([0.]) tensor([0.1448])\n",
      "tensor([0.]) tensor([0.1636])\n",
      "tensor([0.]) tensor([0.1264])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1424])\n",
      "tensor([0.]) tensor([0.1273])\n",
      "tensor([0.]) tensor([0.1683])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1271])\n",
      "tensor([0.]) tensor([0.1548])\n",
      "tensor([0.]) tensor([0.1528])\n",
      "tensor([0.]) tensor([0.1368])\n",
      "tensor([0.]) tensor([0.1391])\n",
      "tensor([0.]) tensor([0.1471])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1862])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.1545])\n",
      "tensor([0.]) tensor([0.1489])\n",
      "tensor([0.]) tensor([0.1420])\n",
      "tensor([0.]) tensor([0.1558])\n",
      "tensor([0.]) tensor([0.2689])\n",
      "tensor([0.]) tensor([0.1413])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1389])\n",
      "tensor([0.]) tensor([0.1507])\n",
      "tensor([0.]) tensor([0.1438])\n",
      "tensor([0.]) tensor([0.1319])\n",
      "tensor([0.]) tensor([0.1569])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.2410])\n",
      "tensor([0.]) tensor([0.1703])\n",
      "tensor([0.]) tensor([0.1474])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1271])\n",
      "tensor([0.]) tensor([0.1557])\n",
      "tensor([0.]) tensor([0.1313])\n",
      "tensor([0.]) tensor([0.1606])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1300])\n",
      "tensor([0.]) tensor([0.1274])\n",
      "tensor([0.]) tensor([0.1310])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1610])\n",
      "tensor([0.]) tensor([0.2069])\n",
      "tensor([0.]) tensor([0.2004])\n",
      "tensor([0.]) tensor([0.1471])\n",
      "tensor([0.]) tensor([0.1503])\n",
      "tensor([0.]) tensor([0.1464])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.1329])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1586])\n",
      "tensor([0.]) tensor([0.1277])\n",
      "tensor([0.]) tensor([0.1675])\n",
      "tensor([0.]) tensor([0.1410])\n",
      "tensor([0.]) tensor([0.1480])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1438])\n",
      "tensor([0.]) tensor([0.1439])\n",
      "tensor([0.]) tensor([0.1564])\n",
      "tensor([0.]) tensor([0.1325])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.1449])\n",
      "tensor([0.]) tensor([0.1432])\n",
      "tensor([0.]) tensor([0.1384])\n",
      "tensor([0.]) tensor([0.1613])\n",
      "tensor([0.]) tensor([0.1356])\n",
      "tensor([0.]) tensor([0.1303])\n",
      "tensor([0.]) tensor([0.1570])\n",
      "tensor([0.]) tensor([0.1731])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1514])\n",
      "tensor([0.]) tensor([0.1601])\n",
      "tensor([0.]) tensor([0.1421])\n",
      "tensor([0.]) tensor([0.1373])\n",
      "tensor([0.]) tensor([0.1318])\n",
      "tensor([0.]) tensor([0.1385])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1428])\n",
      "tensor([0.]) tensor([0.1632])\n",
      "tensor([0.]) tensor([0.1580])\n",
      "tensor([0.]) tensor([0.1461])\n",
      "tensor([0.]) tensor([0.1328])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.1624])\n",
      "tensor([0.]) tensor([0.1292])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1262])\n",
      "tensor([0.]) tensor([0.1285])\n",
      "tensor([0.]) tensor([0.1466])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1646])\n",
      "tensor([0.]) tensor([0.1602])\n",
      "tensor([0.]) tensor([0.1596])\n",
      "tensor([0.]) tensor([0.1561])\n",
      "tensor([0.]) tensor([0.1347])\n",
      "tensor([0.]) tensor([0.1470])\n",
      "tensor([0.]) tensor([0.1354])\n",
      "tensor([0.]) tensor([0.1603])\n",
      "tensor([0.]) tensor([0.1316])\n",
      "tensor([0.]) tensor([0.1451])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1998])\n",
      "tensor([0.]) tensor([0.1314])\n",
      "tensor([0.]) tensor([0.1488])\n",
      "tensor([0.]) tensor([0.1572])\n",
      "tensor([0.]) tensor([0.1439])\n",
      "tensor([0.]) tensor([0.1743])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1445])\n",
      "tensor([0.]) tensor([0.1609])\n",
      "tensor([0.]) tensor([0.1508])\n",
      "tensor([0.]) tensor([0.1275])\n",
      "tensor([0.]) tensor([0.1400])\n",
      "tensor([0.]) tensor([0.1743])\n",
      "tensor([0.]) tensor([0.1656])\n",
      "tensor([0.]) tensor([0.1617])\n",
      "tensor([0.]) tensor([0.1360])\n",
      "tensor([0.]) tensor([0.1626])\n",
      "tensor([0.]) tensor([0.1369])\n",
      "tensor([0.]) tensor([0.1425])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1520])\n",
      "tensor([0.]) tensor([0.1312])\n",
      "tensor([0.]) tensor([0.1783])\n",
      "tensor([0.]) tensor([0.1692])\n",
      "tensor([0.]) tensor([0.1389])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1441])\n",
      "tensor([0.]) tensor([0.1447])\n",
      "tensor([0.]) tensor([0.1586])\n",
      "tensor([0.]) tensor([0.1374])\n",
      "tensor([0.]) tensor([0.1293])\n",
      "tensor([0.]) tensor([0.1670])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.1297])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.1270])\n",
      "tensor([0.]) tensor([0.1326])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.1544])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1559])\n",
      "tensor([0.]) tensor([0.1441])\n",
      "tensor([0.]) tensor([0.1394])\n",
      "tensor([0.]) tensor([0.1543])\n",
      "tensor([0.]) tensor([0.1499])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1482])\n",
      "tensor([0.]) tensor([0.1500])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1437])\n",
      "tensor([0.]) tensor([0.1500])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1268])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1487])\n",
      "tensor([0.]) tensor([0.2728])\n",
      "tensor([0.]) tensor([0.1362])\n",
      "tensor([0.]) tensor([0.1318])\n",
      "tensor([0.]) tensor([0.1468])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1377])\n",
      "tensor([0.]) tensor([0.1342])\n",
      "tensor([0.]) tensor([0.1396])\n",
      "tensor([0.]) tensor([0.1536])\n",
      "tensor([0.]) tensor([0.1398])\n",
      "tensor([0.]) tensor([0.1453])\n",
      "tensor([0.]) tensor([0.1526])\n",
      "tensor([0.]) tensor([0.1975])\n",
      "tensor([0.]) tensor([0.1347])\n",
      "tensor([0.]) tensor([0.1482])\n",
      "tensor([0.]) tensor([0.1505])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.2192])\n",
      "tensor([0.]) tensor([0.1393])\n",
      "tensor([0.]) tensor([0.1260])\n",
      "tensor([0.]) tensor([0.1473])\n",
      "tensor([0.]) tensor([0.1595])\n",
      "tensor([0.]) tensor([0.1284])\n",
      "tensor([0.]) tensor([0.1621])\n",
      "tensor([0.]) tensor([0.1549])\n",
      "tensor([0.]) tensor([0.1293])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1635])\n",
      "tensor([0.]) tensor([0.1271])\n",
      "tensor([0.]) tensor([0.1286])\n",
      "tensor([0.]) tensor([0.1354])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1812])\n",
      "tensor([0.]) tensor([0.1315])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1283])\n",
      "tensor([0.]) tensor([0.1285])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1275])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1359])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1338])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.2342])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1462])\n",
      "tensor([0.]) tensor([0.1434])\n",
      "tensor([0.]) tensor([0.1384])\n",
      "tensor([0.]) tensor([0.1607])\n",
      "tensor([0.]) tensor([0.1486])\n",
      "tensor([0.]) tensor([0.1630])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1450])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1663])\n",
      "tensor([0.]) tensor([0.1469])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1429])\n",
      "tensor([0.]) tensor([0.1442])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1415])\n",
      "tensor([0.]) tensor([0.1287])\n",
      "tensor([0.]) tensor([0.1381])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.1442])\n",
      "tensor([0.]) tensor([0.1512])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1331])\n",
      "tensor([0.]) tensor([0.1496])\n",
      "tensor([0.]) tensor([0.1551])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.1588])\n",
      "tensor([0.]) tensor([0.1649])\n",
      "tensor([0.]) tensor([0.1405])\n",
      "tensor([0.]) tensor([0.1271])\n",
      "tensor([0.]) tensor([0.1382])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1503])\n",
      "tensor([0.]) tensor([0.1519])\n",
      "tensor([0.]) tensor([0.1563])\n",
      "tensor([0.]) tensor([0.1383])\n",
      "tensor([0.]) tensor([0.1371])\n",
      "tensor([0.]) tensor([0.1419])\n",
      "tensor([0.]) tensor([0.1492])\n",
      "tensor([0.]) tensor([0.1607])\n",
      "tensor([0.]) tensor([0.1323])\n",
      "tensor([0.]) tensor([0.2321])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.1379])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.1345])\n",
      "tensor([0.]) tensor([0.1478])\n",
      "tensor([0.]) tensor([0.1621])\n",
      "tensor([0.]) tensor([0.1561])\n",
      "tensor([0.]) tensor([0.1278])\n",
      "tensor([0.]) tensor([0.1376])\n",
      "tensor([0.]) tensor([0.1319])\n",
      "tensor([0.]) tensor([0.1564])\n",
      "tensor([0.]) tensor([0.1463])\n",
      "tensor([0.]) tensor([0.1289])\n",
      "tensor([0.]) tensor([0.1418])\n",
      "tensor([0.]) tensor([0.1269])\n",
      "tensor([0.]) tensor([0.1525])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1395])\n",
      "tensor([0.]) tensor([0.1385])\n",
      "tensor([0.]) tensor([0.1388])\n",
      "tensor([0.]) tensor([0.1517])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1459])\n",
      "tensor([0.]) tensor([0.1265])\n",
      "tensor([0.]) tensor([0.1333])\n",
      "tensor([0.]) tensor([0.1426])\n",
      "tensor([0.]) tensor([0.1409])\n",
      "tensor([0.]) tensor([0.1423])\n",
      "tensor([0.]) tensor([0.1497])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1552])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1377])\n",
      "tensor([0.]) tensor([0.1276])\n",
      "tensor([0.]) tensor([0.1267])\n",
      "tensor([0.]) tensor([0.1393])\n",
      "tensor([0.]) tensor([0.1495])\n",
      "tensor([0.]) tensor([0.1264])\n",
      "tensor([0.]) tensor([0.1417])\n",
      "tensor([0.]) tensor([0.1549])\n",
      "tensor([0.]) tensor([0.1430])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1368])\n",
      "tensor([0.]) tensor([0.1357])\n",
      "tensor([0.]) tensor([0.1543])\n",
      "tensor([0.]) tensor([0.1484])\n",
      "tensor([0.]) tensor([0.1280])\n",
      "tensor([0.]) tensor([0.1569])\n",
      "tensor([0.]) tensor([0.1278])\n",
      "tensor([0.]) tensor([0.1454])\n",
      "tensor([0.]) tensor([0.1290])\n",
      "tensor([0.]) tensor([0.1446])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1393])\n",
      "tensor([0.]) tensor([0.2424])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.2091])\n",
      "tensor([0.]) tensor([0.1266])\n",
      "tensor([0.]) tensor([0.1380])\n",
      "tensor([0.]) tensor([0.1559])\n",
      "tensor([0.]) tensor([0.1340])\n",
      "tensor([0.]) tensor([0.1455])\n",
      "tensor([0.]) tensor([0.1364])\n",
      "tensor([0.]) tensor([0.1530])\n",
      "tensor([0.]) tensor([0.2024])\n",
      "NN Predicttion: (0.906, 0.0, nan, nan)\n",
      "[tensor(0.2555, grad_fn=<MseLossBackward>), tensor(0.1815, grad_fn=<MseLossBackward>), tensor(0.1421, grad_fn=<MseLossBackward>), tensor(0.1211, grad_fn=<MseLossBackward>), tensor(0.1095, grad_fn=<MseLossBackward>), tensor(0.1024, grad_fn=<MseLossBackward>), tensor(0.0977, grad_fn=<MseLossBackward>), tensor(0.0944, grad_fn=<MseLossBackward>), tensor(0.0921, grad_fn=<MseLossBackward>), tensor(0.0903, grad_fn=<MseLossBackward>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:209: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pre-processing of input data\n",
    "predictions = []\n",
    "x_train_torch = pd.DataFrame(X_train)\n",
    "y_train_torch = [int(i) for i in y_train]\n",
    "\n",
    "#training model\n",
    "start=datetime.now()\n",
    "model,loss_li = nnTrain_batch(np.array(x_train_torch,dtype='float32'), y_train_torch,256, 10, 0.05)  #X, Y, batch_size, n_epochs, learning_rate\n",
    "print (\"training compute time \", (datetime.now()-start).total_seconds())\n",
    "\n",
    "#testing set validation\n",
    "for i,item in enumerate(np.array(X_test)):\n",
    "    \n",
    "    item = np.array(item,dtype='float32')\n",
    "    with torch.no_grad():\n",
    "        prediction = model(torch.from_numpy(item))\n",
    "        print(np.round(prediction), prediction)\n",
    "        \n",
    "    # the predicted label is the one has higher probability  \n",
    "    predictions.append(np.round(prediction))\n",
    "  \n",
    "            \n",
    "print(\"NN Predicttion:\",accuracy(y_test, predictions)) #accuray ,precision, recall , f1\n",
    "print(loss_li)\n",
    "\n",
    "#plot loss over epoch\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(loss_li)), loss_li)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting hyperparameters: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318 3580\n",
      "0.001\n",
      "0.005\n",
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.5\n",
      "1\n",
      "5\n",
      "14318 3580\n",
      "0.001\n",
      "0.005\n",
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.5\n",
      "1\n",
      "5\n",
      "14318 3580\n",
      "0.001\n",
      "0.005\n",
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.5\n",
      "1\n",
      "5\n",
      "14319 3579\n",
      "0.001\n",
      "0.005\n",
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.5\n",
      "1\n",
      "5\n",
      "14319 3579\n",
      "0.001\n",
      "0.005\n",
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.5\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "record = []\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "#convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over each fold of training- testing data-set\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    print(\"training data-set size\",len(train_index), \"testing data-set size\",len(test_index))\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #training for each learning rate\n",
    "    learning_rate = [0.001,0.005, 0.01, 0.05, 0.1, 0.5, 1,5]\n",
    "    for l in learning_rate:\n",
    "        print(\"learning rate:\", l)\n",
    "        # traininig and testing\n",
    "        net = Network([8, 10, 1])\n",
    "        net.SGD(training_data, 10, 256, l, test_data=test_data) # epochs, batch_size, learning rate\n",
    "        for data in net.cost_li:\n",
    "            record.append(data)\n",
    "                   \n",
    "# save output\n",
    "record_df = pd.DataFrame(record, columns=['epoch', 'batch_size','loss', 'learning_rate','accuracy_test','accuracy_train'])\n",
    "record_df.to_csv(\"learning_rate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting hyperparameters: hidden neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318 3580\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "14318 3580\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "14318 3580\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "14319 3579\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "14319 3579\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "record = []\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "#convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over each fold of training- testing data-set\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    print(\"training data-set size\",len(train_index), \"testing data-set size\",len(test_index))\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # test number of hidden neurons from 10 to 100, increase by 10, and collect loss, accuracy data\n",
    "    for n in range(10,110,10):\n",
    "        print(\"hidden neurons: \",n)\n",
    "        # traininig and testing\n",
    "        net = Network([8, n, 1])\n",
    "        net.SGD(training_data, 10, 256, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "        cost = net.total_cost(test_data)\n",
    "        accuracy_test = net.predict(test_data)[0]/len(test_data)\n",
    "        tmp =[n, cost[0][0], accuracy_test]\n",
    "        record.append(tmp)\n",
    "\n",
    "# save output\n",
    "record_df = pd.DataFrame(record, columns=['hidden_neurons', 'avg.loss','avg.accuracy'])\n",
    "record_df.to_csv(\"hidden_neurons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting hyperparameters: batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318 3580\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "450\n",
      "14318 3580\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "450\n",
      "14318 3580\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "450\n",
      "14319 3579\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "450\n",
      "14319 3579\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "record = []\n",
    "batch_size = [4,8,16,32, 64, 128, 256, 450]\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "#convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over each fold of training- testing data-set\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    print(\"training data-set size\",len(train_index), \"testing data-set size\",len(test_index))\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # test number of hidden neurons from 10 to 100, increase by 10, and collect loss, accuracy data\n",
    "    for b in batch_size:\n",
    "        print(\"batch_size:\",b)\n",
    "        # traininig and testing\n",
    "        net = Network([8, 60, 1])\n",
    "        net.SGD(training_data, 10, b, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "        \n",
    "        for data in net.cost_li:\n",
    "            record.append(data)\n",
    "\n",
    "# save output\n",
    "record_df = pd.DataFrame(record, columns=['epoch', 'batch_size','loss', 'learning_rate','accuracy_test','accuracy_train'])\n",
    "record_df.to_csv(\"batch_size.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318 3580\n",
      "14318 3580\n",
      "14318 3580\n",
      "14319 3579\n",
      "14319 3579\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "record = []\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "#convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over each fold of training- testing data-set\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    print(\"training data-set size\",len(train_index), \"testing data-set size\",len(test_index))\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "    # traininig and testing\n",
    "    net = Network([8, 60, 1])\n",
    "    net.SGD(training_data, 100, 32, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "    for data in net.cost_li:\n",
    "        record.append(data)\n",
    "\n",
    "# save output\n",
    "record_df = pd.DataFrame(record, columns=['epoch', 'batch_size','loss', 'learning_rate','accuracy_test','accuracy_train'])\n",
    "record_df.to_csv(\"epochs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final model : cross-validation / ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14318 3580\n",
      "14318 3580\n",
      "14318 3580\n",
      "14319 3579\n",
      "14319 3579\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 0.05, batch_size = 32, epoch = 20\n",
    "\n",
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "record = []\n",
    "prob= []\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "# convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over \n",
    "loop =1\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    print(\"training data-set size\",len(train_index), \"testing data-set size\",len(test_index))\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "    # traininig and testing\n",
    "    net = Network([8, 60, 1])\n",
    "    net.SGD(training_data, 20, 32, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "    cost = net.total_cost(test_data)\n",
    "    #validation\n",
    "    accurate,results = net.predict(test_data)\n",
    "    # un-tuple the results\n",
    "    results_unnest = [(int(x[0][0]), int(y)) for (x,y) in results]\n",
    "    results_df = pd.DataFrame(results_unnest) # pred, y\n",
    "    accuracy_test ,precision, recall ,f1 =  accuracy(results_df[0], results_df[1]) # pred, y\n",
    "    \n",
    "    #predicted probability, for roc curve\n",
    "    pred_prob = net.predict_prob(test_data)\n",
    "    prob_unnest = [(x[0][0], y) for (x,y) in pred_prob]\n",
    "    prob_df = pd.DataFrame(prob_unnest) # pred, y\n",
    "    \n",
    "    # save data into list\n",
    "    tmp =[loop,cost[0][0], accuracy_test ,precision, recall ,f1]\n",
    "    record.append(tmp)\n",
    "    for i in range(len(prob_df[0])):\n",
    "        tmp_prob =[loop, prob_df[0][i],  prob_df[1][i]]\n",
    "        prob.append(tmp_prob)\n",
    "    \n",
    "    loop+=1\n",
    "     \n",
    "# save outputs\n",
    "record_df = pd.DataFrame(record, columns=['k_fold','cost', 'accuracy' ,'precision', 'recall' ,'f1'])\n",
    "record_df.to_csv(\"final_model.csv\")\n",
    "\n",
    "output_df = pd.DataFrame(prob, columns=['k_fold','pred_prob', 'y'])\n",
    "output_df.to_csv(\"final_model_prob.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparison : time consuming over each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold: 1 compute time: 18.858138\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 1 compute time: 8.656911\n",
      "k-fold: 2 compute time: 18.544591\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 2 compute time: 8.633935\n",
      "k-fold: 3 compute time: 18.592806\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 3 compute time: 8.694403\n",
      "k-fold: 4 compute time: 18.811648\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 4 compute time: 8.837635\n",
      "k-fold: 5 compute time: 19.067195\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 5 compute time: 8.715353\n",
      "k-fold: 6 compute time: 20.123933\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 6 compute time: 8.783012\n",
      "k-fold: 7 compute time: 18.97587\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 7 compute time: 8.842991\n",
      "k-fold: 8 compute time: 19.40069\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 8 compute time: 9.328917\n",
      "k-fold: 9 compute time: 20.272448\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 9 compute time: 9.079383\n",
      "k-fold: 10 compute time: 19.475977\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "k-fold: 10 compute time: 8.792552\n",
      "[[1, 'pytorch', 8.656911], [1, 'nn_application', 18.858138], [2, 'pytorch', 8.633935], [2, 'nn_application', 18.544591], [3, 'pytorch', 8.694403], [3, 'nn_application', 18.592806], [4, 'pytorch', 8.837635], [4, 'nn_application', 18.811648], [5, 'pytorch', 8.715353], [5, 'nn_application', 19.067195], [6, 'pytorch', 8.783012], [6, 'nn_application', 20.123933], [7, 'pytorch', 8.842991], [7, 'nn_application', 18.97587], [8, 'pytorch', 9.328917], [8, 'nn_application', 19.40069], [9, 'pytorch', 9.079383], [9, 'nn_application', 20.272448], [10, 'pytorch', 8.792552], [10, 'nn_application', 19.475977]]\n",
      "[7.13075, 7.426442, 7.067935, 7.208012, 7.130816]\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 0.05, batch_size = 32, epoch = 20\n",
    "\n",
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "time_nn = []\n",
    "\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "# convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over 10-fold data\n",
    "loop =1\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # traininig : using nn implement \n",
    "    start=datetime.now()\n",
    "    net = Network([8, 60, 1])\n",
    "    net.SGD(training_data, 20, 32, 0.05, test_data=None) # epochs, batch_size, learning rate\n",
    "    nn_time = (datetime.now()-start).total_seconds()\n",
    "    print(\"k-fold:\",loop, \"compute time:\",nn_time)\n",
    "    \n",
    "    # traininig :  pytorch \n",
    "    x_train_torch = pd.DataFrame(X_train)\n",
    "    y_train_torch = [int(i) for i in y_train]\n",
    "    start=datetime.now()\n",
    "    model,loss_li = nnTrain_batch(np.array(x_train_torch,dtype='float32'), y_train_torch,32, 20, 0.05) #X, Y, batch_size, n_epochs, learning_rate\n",
    "    pytorch_time = (datetime.now()-start).total_seconds()\n",
    "    print(\"k-fold:\",loop, \"compute time:\", pytorch_time)\n",
    "    time_nn.append([loop,'pytorch',pytorch_time])\n",
    "    time_nn.append([loop,'nn_application',nn_time])\n",
    "    \n",
    "    loop +=1\n",
    "\n",
    "print(time_nn)\n",
    "print(time_pytorch)\n",
    "\n",
    "time_nn_df = pd.DataFrame(time_nn, columns=['cv','model_type','compute_time'])\n",
    "time_nn_df.to_csv(\"time_nn.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparison : time consuming using different size of hidden nerons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold: 11 compute time: 16.114751\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 10 compute time: 7.520434\n",
      "k-fold: 11 compute time: 16.169184\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 20 compute time: 7.450844\n",
      "k-fold: 11 compute time: 16.484936\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 30 compute time: 7.488768\n",
      "k-fold: 11 compute time: 16.610229\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 40 compute time: 7.696672\n",
      "k-fold: 11 compute time: 16.993996\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 50 compute time: 7.87963\n",
      "k-fold: 11 compute time: 17.371252\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 60 compute time: 7.495198\n",
      "k-fold: 11 compute time: 16.900965\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 70 compute time: 8.012652\n",
      "k-fold: 11 compute time: 17.479987\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 80 compute time: 7.674433\n",
      "k-fold: 11 compute time: 17.66271\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 90 compute time: 7.789711\n",
      "k-fold: 11 compute time: 17.878793\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "hidden neurons: 100 compute time: 7.723621\n",
      "[[10, 'pytorch', 7.520434], [10, 'nn_application', 16.114751], [20, 'pytorch', 7.450844], [20, 'nn_application', 16.169184], [30, 'pytorch', 7.488768], [30, 'nn_application', 16.484936], [40, 'pytorch', 7.696672], [40, 'nn_application', 16.610229], [50, 'pytorch', 7.87963], [50, 'nn_application', 16.993996], [60, 'pytorch', 7.495198], [60, 'nn_application', 17.371252], [70, 'pytorch', 8.012652], [70, 'nn_application', 16.900965], [80, 'pytorch', 7.674433], [80, 'nn_application', 17.479987], [90, 'pytorch', 7.789711], [90, 'nn_application', 17.66271], [100, 'pytorch', 7.723621], [100, 'nn_application', 17.878793]]\n",
      "[7.13075, 7.426442, 7.067935, 7.208012, 7.130816]\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 0.05, batch_size = 32, epoch = 20\n",
    "\n",
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels , test_size=0.2)\n",
    "x_train_torch = pd.DataFrame(X_train)\n",
    "y_train_torch = [int(i) for i in y_train]\n",
    "training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "\n",
    "time_nn = []\n",
    "\n",
    "# loop over each setting of hidden neurons\n",
    "for n in range(10,110,10):\n",
    "\n",
    "    # traininig : using nn implement \n",
    "    start=datetime.now()\n",
    "    net = Network([8, n, 1])\n",
    "    net.SGD(training_data, 20, 32, 0.05, test_data=None) # epochs, batch_size, learning rate\n",
    "    nn_time = (datetime.now()-start).total_seconds()\n",
    "    print(\"k-fold:\",n, \"compute time:\",nn_time)\n",
    "    \n",
    "    # traininig :  pytorch \n",
    "    start=datetime.now()\n",
    "    model,loss_li = nnTrain_batch(np.array(x_train_torch,dtype='float32'), y_train_torch,32, 20, 0.05, h =n) #X, Y, batch_size, n_epochs, learning_rate\n",
    "    pytorch_time = (datetime.now()-start).total_seconds()\n",
    "    print(\"hidden neurons:\",n, \"compute time:\", pytorch_time)\n",
    "    time_nn.append([n,'pytorch',pytorch_time])\n",
    "    time_nn.append([n,'nn_application',nn_time])\n",
    "    \n",
    "\n",
    "print(time_nn)\n",
    "print(time_pytorch)\n",
    "\n",
    "time_nn_df = pd.DataFrame(time_nn, columns=['neurons','model_type','compute_time'])\n",
    "time_nn_df.to_csv(\"time_nn_hidden_neurons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparison : model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:209: RuntimeWarning: invalid value encountered in long_scalars\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:208: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:210: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n",
      "epoch:  0  finish\n",
      "epoch:  1  finish\n",
      "epoch:  2  finish\n",
      "epoch:  3  finish\n",
      "epoch:  4  finish\n",
      "epoch:  5  finish\n",
      "epoch:  6  finish\n",
      "epoch:  7  finish\n",
      "epoch:  8  finish\n",
      "epoch:  9  finish\n",
      "epoch:  10  finish\n",
      "epoch:  11  finish\n",
      "epoch:  12  finish\n",
      "epoch:  13  finish\n",
      "epoch:  14  finish\n",
      "epoch:  15  finish\n",
      "epoch:  16  finish\n",
      "epoch:  17  finish\n",
      "epoch:  18  finish\n",
      "epoch:  19  finish\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 0.05, batch_size = 32, epoch = 20\n",
    "\n",
    "#load data\n",
    "data,labels = loadData(\"HTRU_2.csv\")\n",
    "model_output_comparison = []\n",
    "\n",
    "\n",
    "# collecting data from 5-fold data\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "# convert data into nparray\n",
    "np_data = np.array(data)\n",
    "np_labels = np.array(labels)\n",
    "\n",
    "# loop over \n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    # split data into training and testing data set\n",
    "    X_train, X_test, y_train, y_test = np_data[train_index], np_data[test_index], np_labels[train_index], np_labels[test_index]\n",
    "    training_data, test_data = zip_data( X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # traininig : using nn implement \n",
    "    net = Network([8, 60, 1])\n",
    "    net.SGD(training_data, 20, 32, 0.05, test_data=None) # epochs, batch_size, learning rate\n",
    "    \n",
    "    #validation\n",
    "    accurate,results = net.predict(test_data)\n",
    "    # un-tuple the results\n",
    "    results_unnest = [(int(x[0][0]), int(y)) for (x,y) in results]\n",
    "    results_df = pd.DataFrame(results_unnest) # pred, y\n",
    "    accuracy_test ,precision, recall ,f1 =  accuracy(results_df[0], results_df[1]) # pred, y\n",
    "    \n",
    "    #collect data\n",
    "    model_output_comparison.append([\"nn\",\"accuracy\",accuracy_test])\n",
    "    model_output_comparison.append([\"nn\",\"precision\", precision])\n",
    "    model_output_comparison.append([\"nn\",\"recall\",recall])\n",
    "    model_output_comparison.append([\"nn\",\"f1\",f1])\n",
    "    \n",
    "    # traininig :  pytorch \n",
    "    x_train_torch = pd.DataFrame(X_train)\n",
    "    y_train_torch = [int(i) for i in y_train]\n",
    "    model,loss_li = nnTrain_batch(np.array(x_train_torch,dtype='float32'), y_train_torch,32, 20, 0.05) #X, Y, batch_size, n_epochs, learning_rate\n",
    "    \n",
    "    #validation\n",
    "    predictions=[]\n",
    "    for i,item in enumerate(np.array(X_test)):  \n",
    "       \n",
    "        item = np.array(item,dtype='float32')\n",
    "        with torch.no_grad():\n",
    "            prediction = model(torch.from_numpy(item))\n",
    "        \n",
    "        # the predicted label is the one has higher probability  \n",
    "        predictions.append(np.round(prediction))\n",
    "  \n",
    "            \n",
    "    accuracy_test_p ,precision_p, recall_p ,f1_p = accuracy(y_test, predictions)\n",
    "\n",
    "    \n",
    "    #collect data\n",
    "    model_output_comparison.append([\"pytorch\",\"accuracy\",accuracy_test_p])\n",
    "    model_output_comparison.append([\"pytorch\",\"precision\", precision_p])\n",
    "    model_output_comparison.append([\"pytorch\",\"recall\",recall_p])\n",
    "    model_output_comparison.append([\"pytorch\",\"f1\",f1_p])\n",
    "\n",
    "comparison_df = pd.DataFrame(model_output_comparison, columns=['model','evaluation','value'])\n",
    "comparison_df.to_csv(\"comparison_output.csv\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------weights, biases, loss of the training model---------------\n",
      "Epoch 0 complete, loss : [[0.78345128]]\n",
      "Epoch 1 complete, loss : [[0.77920083]]\n",
      "Epoch 2 complete, loss : [[0.7748693]]\n",
      "Epoch 3 complete, loss : [[0.77045768]]\n",
      "Epoch 4 complete, loss : [[0.76596719]]\n",
      "Epoch 5 complete, loss : [[0.76139933]]\n",
      "Epoch 6 complete, loss : [[0.75675585]]\n",
      "Epoch 7 complete, loss : [[0.75203876]]\n",
      "Epoch 8 complete, loss : [[0.74725036]]\n",
      "Epoch 9 complete, loss : [[0.74239321]]\n",
      "Epoch 10 complete, loss : [[0.73747014]]\n",
      "Epoch 11 complete, loss : [[0.73248424]]\n",
      "Epoch 12 complete, loss : [[0.72743886]]\n",
      "Epoch 13 complete, loss : [[0.72233759]]\n",
      "Epoch 14 complete, loss : [[0.71718427]]\n",
      "Epoch 15 complete, loss : [[0.71198294]]\n",
      "Epoch 16 complete, loss : [[0.70673782]]\n",
      "Epoch 17 complete, loss : [[0.70145335]]\n",
      "Epoch 18 complete, loss : [[0.69613405]]\n",
      "Epoch 19 complete, loss : [[0.69078463]]\n",
      "biases:   [array([[ 0.8786868 ],\n",
      "       [-0.53034752],\n",
      "       [-0.03212396],\n",
      "       [-0.47314831],\n",
      "       [-0.91819468],\n",
      "       [-0.99397459],\n",
      "       [ 0.17292763],\n",
      "       [-0.86256719],\n",
      "       [ 1.07683117],\n",
      "       [ 0.07909348]]), array([[-1.5619976]])]\n",
      "weights:   [array([[-2.01081647e+00, -2.51858305e-01, -1.43341298e+00,\n",
      "        -1.35781016e+00, -3.25491884e-01, -1.08684885e+00,\n",
      "        -6.08249066e-01, -1.14669944e+00],\n",
      "       [-1.71647309e-01,  1.69190530e+00,  5.53747363e-01,\n",
      "         6.75021794e-01, -2.39786108e-01, -5.15365503e-01,\n",
      "         9.75132613e-01,  5.38775371e-01],\n",
      "       [-2.70783021e+00, -1.26999768e+00, -3.64221808e-01,\n",
      "         2.01217881e-01,  9.51991861e-01, -3.03344446e-01,\n",
      "         1.32302653e+00,  1.63370014e+00],\n",
      "       [ 5.43127366e-01,  4.52742305e-01,  5.14845142e-01,\n",
      "        -5.77441896e-01, -1.03023019e-01,  2.00065218e+00,\n",
      "         9.74101447e-01,  7.33764386e-01],\n",
      "       [ 5.65343888e-01,  8.61273589e-01, -4.72666080e-01,\n",
      "        -1.98778879e-01,  6.60595668e-01, -1.90711106e+00,\n",
      "         9.78901507e-03, -1.35986704e+00],\n",
      "       [ 2.08697112e+00,  1.39882850e+00,  1.13478074e-01,\n",
      "        -2.19576138e-01,  3.94678807e-01, -1.81151308e+00,\n",
      "        -6.92218105e-01, -4.58909787e-01],\n",
      "       [-2.46366951e+00, -4.79541111e-01,  5.66130261e-01,\n",
      "         9.79351634e-02, -7.81027846e-01, -2.01117051e+00,\n",
      "         2.06097401e-01, -1.32943103e+00],\n",
      "       [-1.14425219e+00,  6.83091816e-01, -1.13252594e-01,\n",
      "         2.15509909e-03,  8.07666852e-01, -6.01984915e-02,\n",
      "        -2.92532382e+00,  1.21249958e+00],\n",
      "       [-2.50720564e-01, -7.08905104e-01, -2.40990542e+00,\n",
      "        -3.15091507e-03, -3.43148581e-02,  8.63532953e-01,\n",
      "         1.81143014e+00, -2.41687816e-01],\n",
      "       [ 9.93712760e-01,  2.50534425e-01,  1.40692839e+00,\n",
      "         2.70000153e-01,  8.75562311e-01,  1.22323369e-01,\n",
      "        -1.09430067e+00, -1.73628607e+00]]), array([[ 1.03217207,  1.82741943,  0.50477164,  2.33453306,  0.15402185,\n",
      "         0.92387389, -1.2862315 ,  1.45430242,  0.70297514, -0.78437861]])]\n",
      "----------predict test data---------------\n",
      "predict probabilities(probabilites, true label):  [(array([[0.90912918]]), 1), (array([[0.85232643]]), 0)]\n",
      "predict results(pred, true label):   [(array([[1.]]), 1), (array([[1.]]), 0)]\n",
      "number of correct predictions:   1\n",
      "----------accuracy, precision, recall, f1 score---------------\n",
      "accuracy:   0.5\n",
      "precision:   0.5\n",
      "recall:   1.0\n",
      "f1:   0.667\n"
     ]
    }
   ],
   "source": [
    "# generate 2 training data and 2 testing data, total input variables are 8.\n",
    "test_size = 2\n",
    "input_nodes = 8\n",
    "test_X_train = [np.random.randn(1, input_nodes) for y in range(test_size)]\n",
    "test_X_test  = [np.random.randn(1, input_nodes) for y in range(test_size)]\n",
    "test_y_train = np.random.randint(2, size=test_size) \n",
    "test_y_test  = np.random.randint(2, size=test_size) \n",
    "\n",
    "#convert data into tuple\n",
    "\n",
    "training_data = list(zip(test_X_train, test_y_train))\n",
    "test_data = list(zip(test_X_test, test_y_test))\n",
    "\n",
    "\n",
    "print(\"----------weights, biases, loss of the training model---------------\")\n",
    "\n",
    "# train the model\n",
    "net = Network([8, 10, 1])\n",
    "net.SGD(training_data, 20, 32, 0.05, test_data=test_data) # epochs, batch_size, learning rate\n",
    "print(\"biases:  \",net.biases)\n",
    "print(\"weights:  \",net.weights)\n",
    "\n",
    "print(\"----------predict test data---------------\")\n",
    "\n",
    "prob_results= net.predict_prob(test_data)\n",
    "accurate,results = net.predict(test_data)\n",
    "print(\"predict probabilities(probabilites, true label): \", prob_results)\n",
    "print(\"predict results(pred, true label):  \", results)\n",
    "print(\"number of correct predictions:  \", accurate)\n",
    "\n",
    "\n",
    "print(\"----------accuracy, precision, recall, f1 score---------------\")\n",
    "\n",
    "# un-tuple the results\n",
    "results_unnest = [(int(x[0][0]), int(y)) for (x,y) in results]\n",
    "results_df = pd.DataFrame(results_unnest) # pred, y\n",
    "accuracy_test ,precision, recall ,f1 =  accuracy(results_df[0], results_df[1]) # pred, y\n",
    "print(\"accuracy:  \",accuracy_test)\n",
    "print(\"precision:  \",precision)\n",
    "print(\"recall:  \",recall)\n",
    "print(\"f1:  \",f1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels , test_size=0.2)\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(y_train[0])\n",
    "print(y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
